{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d046916f-63a2-455f-a180-f8c8935e79af",
   "metadata": {},
   "source": [
    "### Linear Congruential Generator (LCG)\n",
    "\n",
    "#### Purpose:\n",
    "Generates pseudo-random numbers using the **Linear Congruential Generator** method.\n",
    "\n",
    "#### Algorithm:\n",
    "The LCG generates numbers \\( u_n \\) based on the recurrence relation:\n",
    "\n",
    "$$\n",
    "u_{n+1} = (a \\cdot u_n + b) \\mod M\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ M = 2^{32} $: Modulus (a large number for full period generation),  \n",
    "- $ a = 1664525 $: Multiplier,  \n",
    "- $ b = 1013904223 $: Increment,  \n",
    "- $ u_0 $: Initial seed.\n",
    "\n",
    "#### Output:\n",
    "The generated pseudo-random numbers are in the range:\n",
    "\n",
    "$$\n",
    "\\{0, 1, 2, \\dots, M-1\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Uniform \\([0, 1]\\) Generator\n",
    "\n",
    "#### Purpose:\n",
    "Converts the pseudo-random numbers generated by the LCG into samples from a **Uniform \\([0, 1]\\)** distribution.\n",
    "\n",
    "#### Algorithm:\n",
    "Let the generator output $ u_n $ in the range $ \\{0, 1, \\dots, M-1\\} $. The uniform samples are obtained by normalizing $ u_n $:\n",
    "\n",
    "$$\n",
    "x_n = \\frac{u_n}{\\text{period}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\text{period} = M $ (from the LCG),  \n",
    "- $ u_n $: Output of the LCG.\n",
    "\n",
    "#### Output:\n",
    "The generated uniform random numbers are in the range:\n",
    "\n",
    "$$\n",
    "[0, 1].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Accept-Reject Sampler\n",
    "\n",
    "#### Purpose:\n",
    "Generates samples from the target distribution:\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{\\pi}{2} \\cdot |\\sin(2\\pi x)|, \\quad x \\in [0, 1].\n",
    "$$\n",
    "\n",
    "This is done using the **Accept-Reject Sampling** algorithm with the **Uniform \\([0, 1]\\)** distribution as the proposal distribution $ g(x) $.\n",
    "\n",
    "#### Algorithm:\n",
    "1. Define the target density $ f(x) = \\frac{\\pi}{2} |\\sin(2\\pi x)| $.  \n",
    "2. Choose the proposal density $ g(x) = 1 $ (uniform distribution) and compute the scaling constant $ M = \\frac{\\pi}{2} $ such that:\n",
    "\n",
    "$$\n",
    "f(x) \\leq M \\cdot g(x), \\quad \\forall x \\in [0, 1].\n",
    "$$\n",
    "\n",
    "3. For each iteration:\n",
    "   - Sample $ x \\sim g(x) $ using the uniform generator.\n",
    "   - Compute the acceptance ratio:\n",
    "\n",
    "$$\n",
    "r(x) = \\frac{f(x)}{M \\cdot g(x)} = |\\sin(2\\pi x)|.\n",
    "$$\n",
    "\n",
    "   - Sample $ U \\sim \\text{Uniform}[0, 1] $ and accept $ x $ if:\n",
    "\n",
    "$$\n",
    "U \\leq r(x).\n",
    "$$\n",
    "\n",
    "4. Repeat until the desired number of accepted samples is reached.\n",
    "\n",
    "#### Output:\n",
    "The accepted samples $ \\{x_i\\} $ follow the target distribution $ p(x) $.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Equations\n",
    "\n",
    "#### 1. Linear Congruential Generator:\n",
    "$$\n",
    "u_{n+1} = (a \\cdot u_n + b) \\mod M\n",
    "$$\n",
    "\n",
    "#### 2. Uniform \\([0, 1]\\) Generator:\n",
    "$$\n",
    "x_n = \\frac{u_n}{\\text{period}}, \\quad \\text{period} = M.\n",
    "$$\n",
    "\n",
    "#### 3. Accept-Reject Sampler:\n",
    "- Target density:\n",
    "  $$\n",
    "  f(x) = \\frac{\\pi}{2} |\\sin(2\\pi x)|.\n",
    "  $$\n",
    "- Proposal density:\n",
    "  $$\n",
    "  g(x) = 1.\n",
    "  $$\n",
    "- Acceptance ratio:\n",
    "  $$\n",
    "  r(x) = \\frac{f(x)}{M \\cdot g(x)} = |\\sin(2\\pi x)|.\n",
    "  $$\n",
    "- Acceptance condition:\n",
    "  $$\n",
    "  U \\leq r(x), \\quad U \\sim \\text{Uniform}[0, 1].\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53c931-1adf-4df2-9dee-7899c41a9840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eee2f7-107f-46fd-8c1b-917a4f98c010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a603a3-cc51-4f4e-8578-cfc3d74cc644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd16e5-d144-4874-9b5b-5fad40823288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a0c88-e91f-45dd-a66f-e47b8d1a462c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9283b31-60f3-4434-98da-5c4148821e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb06b5b-9c2e-4a26-b24e-41a3189e6547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8235950e-d10d-4592-9843-66150b5d8a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  0),\n",
       " ('Ok lar... Joking wif u oni...', 0)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils import load_sms \n",
    "sms_data = load_sms()\n",
    "sms_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c17fe6c3-5c70-48b5-a104-d4b3383a5038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.812)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find SMS with \"free\" or \"prize\"\n",
    "spam_words = set(['free', 'prize'])\n",
    "filtered_sms = [\n",
    "    label for line, label in sms_data\n",
    "    if not spam_words.isdisjoint([word.lower() for word in line.split(' ')])\n",
    "]\n",
    "\n",
    "# Calculate conditional probability\n",
    "problem4_hatP = np.mean(filtered_sms)\n",
    "problem4_hatP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "61147306-235a-425e-b271-e21c20952494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7956042628455681), np.float64(0.828395737154432))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def epsilon_bernoulli(n,alpha):\n",
    "    return np.sqrt(-1/(2*n)*np.log((alpha)/2))\n",
    "\n",
    "epsilon = epsilon_bernoulli(len(Y_obs),0.1)\n",
    "\n",
    "# fill in the calculated l from part 2 here\n",
    "problem4_l = (problem4_hatP-epsilon,problem4_hatP+epsilon)\n",
    "problem4_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fc174529-d635-44b5-a4bc-b585564fb94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in the estimate for hatP for the double free question in part 3 here (should be a number between 0 and 1)\n",
    "\n",
    "free_twice_count = 0\n",
    "spam_free_twice_count = 0\n",
    "for line, label in sms_data:\n",
    "    if line.lower().split(' ').count('free') == 2:\n",
    "        free_twice_count += 1\n",
    "        if label == 1:\n",
    "            spam_free_twice_count += 1\n",
    "            \n",
    "    \n",
    "problem4_hatP2 = spam_free_twice_count / free_twice_count\n",
    "problem4_hatP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6cfef0e5-7397-4db4-9fb9-918d4b048842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.9002709295122346), np.float64(0.9330624038210986))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in the estimate for l for the double free question in part 3 here\n",
    "problem4_l2 = (problem4_hatP2-epsilon,problem4_hatP2+epsilon)\n",
    "problem4_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49b3f7-e25d-4bca-b9bf-0945de2a2005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcc214-9f12-4924-a512-626bdc7ae546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc7f14-dc55-40b4-b2d4-a6a9016e32cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e437ca-6214-4630-83c7-d23712ae3cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a154427-2da9-4d6a-8b24-17143c8e4dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36acff1-e6bb-4dc1-98e4-3290b5e1d32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a34fedf-ed14-4aa5-82f8-6a233d3fcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFreqDict(myDataList):\n",
    "    '''Make a frequency mapping out of a list of data.\n",
    "    Param myDataList, a list of data.\n",
    "    Return a dictionary mapping each unique data value to its frequency count.\n",
    "    '''\n",
    "    freqDict = {} # start with an empty dictionary\n",
    "    for res in myDataList:\n",
    "        if res in freqDict: # the data value already exists as a key\n",
    "            freqDict[res] = freqDict[res] + 1 # add 1 to the count using sage integers\n",
    "        else: # the data value does not exist as a key value\n",
    "            freqDict[res] = 1 # add a new key-value pair for this new data value, frequency 1\n",
    "    \n",
    "    return freqDict # return the dictionary created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4864b34-01df-4a62-9853-68028660b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8a2a5f-b2ca-490c-8222-db6dd3effdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv('flights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd346bb5-281a-4602-96dc-9e9b0073e241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>travelCode</th>\n",
       "      <th>userCode</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>flightType</th>\n",
       "      <th>price</th>\n",
       "      <th>time</th>\n",
       "      <th>distance</th>\n",
       "      <th>agency</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recife (PE)</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1434.38</td>\n",
       "      <td>1.76</td>\n",
       "      <td>676.53</td>\n",
       "      <td>FlyingDrops</td>\n",
       "      <td>09/26/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>Recife (PE)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1292.29</td>\n",
       "      <td>1.76</td>\n",
       "      <td>676.53</td>\n",
       "      <td>FlyingDrops</td>\n",
       "      <td>09/30/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Brasilia (DF)</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1487.52</td>\n",
       "      <td>1.66</td>\n",
       "      <td>637.56</td>\n",
       "      <td>CloudFy</td>\n",
       "      <td>10/03/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Florianopolis (SC)</td>\n",
       "      <td>Brasilia (DF)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1127.36</td>\n",
       "      <td>1.66</td>\n",
       "      <td>637.56</td>\n",
       "      <td>CloudFy</td>\n",
       "      <td>10/04/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Aracaju (SE)</td>\n",
       "      <td>Salvador (BH)</td>\n",
       "      <td>firstClass</td>\n",
       "      <td>1684.05</td>\n",
       "      <td>2.16</td>\n",
       "      <td>830.86</td>\n",
       "      <td>CloudFy</td>\n",
       "      <td>10/10/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   travelCode  userCode                from                  to  flightType  \\\n",
       "0           0         0         Recife (PE)  Florianopolis (SC)  firstClass   \n",
       "1           0         0  Florianopolis (SC)         Recife (PE)  firstClass   \n",
       "2           1         0       Brasilia (DF)  Florianopolis (SC)  firstClass   \n",
       "3           1         0  Florianopolis (SC)       Brasilia (DF)  firstClass   \n",
       "4           2         0        Aracaju (SE)       Salvador (BH)  firstClass   \n",
       "\n",
       "     price  time  distance       agency        date  \n",
       "0  1434.38  1.76    676.53  FlyingDrops  09/26/2019  \n",
       "1  1292.29  1.76    676.53  FlyingDrops  09/30/2019  \n",
       "2  1487.52  1.66    637.56      CloudFy  10/03/2019  \n",
       "3  1127.36  1.66    637.56      CloudFy  10/04/2019  \n",
       "4  1684.05  2.16    830.86      CloudFy  10/10/2019  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca9e9acf-373b-4d60-a610-83ef8447e356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271888, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f206e1d9-9265-4f72-81f1-4b744fd2a446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Recife (PE)', 'Florianopolis (SC)', 'Brasilia (DF)',\n",
       "       'Aracaju (SE)', 'Salvador (BH)', 'Campo Grande (MS)',\n",
       "       'Sao Paulo (SP)', 'Natal (RN)', 'Rio de Janeiro (RJ)'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights['from'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2433b456-d3bd-4931-97c2-c3bd97bdd8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Florianopolis (SC)', 'Recife (PE)', 'Brasilia (DF)',\n",
       "       'Salvador (BH)', 'Aracaju (SE)', 'Campo Grande (MS)',\n",
       "       'Sao Paulo (SP)', 'Natal (RN)', 'Rio de Janeiro (RJ)'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights['to'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f954668-c52c-4579-a7b0-8d5267865bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_cities = len(flights['to'].unique())\n",
    "number_of_userCodes = len(flights['userCode'].unique())\n",
    "number_of_observations = flights.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cd83bfa-8b31-4985-98e5-2e7262c09acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = flights['to']\n",
    "unique_cities = sorted(set(cities)) # The unique cities\n",
    "n_cities = len(unique_cities) # The number of unique citites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c37896ad-52e4-4559-936e-17b43306eda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aracaju (SE)',\n",
       " 'Brasilia (DF)',\n",
       " 'Campo Grande (MS)',\n",
       " 'Florianopolis (SC)',\n",
       " 'Natal (RN)',\n",
       " 'Recife (PE)',\n",
       " 'Rio de Janeiro (RJ)',\n",
       " 'Salvador (BH)',\n",
       " 'Sao Paulo (SP)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f9e17cc-b871-4e9d-8589-0f96540bd6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the different transitions\n",
    "\n",
    "# A list containing tuples ex: ('Aracaju (SE)','Rio de Janeiro (RJ)') of all transitions in the text\n",
    "transitions = list(zip(flights['from'], flights['to']))\n",
    "\n",
    "# A dictionary that counts the number of each transition\n",
    "transition_counts = makeFreqDict(transitions)\n",
    "# ex: ('Aracaju (SE)','Rio de Janeiro (RJ)'):4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6423eb32-19e6-4c1e-9cc8-f4a9ed3539e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexToCity = {index: city for index, city in enumerate(unique_cities)} # A dictionary that maps the n-1 number to the n:th unique_city,\n",
    "# ex: 0:'Aracaju (SE)'\n",
    "\n",
    "cityToIndex = {city: index for index, city in enumerate(unique_cities)} # The inverse function of indexToWord,\n",
    "# ex: 'Aracaju (SE)':0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14adb84e-d1ed-43e6-8a56-a806814522ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0., 4833., 5393., 8643., 3937., 4884., 2818., 2918., 3798.],\n",
       "       [4833.,    0., 4517., 7779., 2987., 3822., 1994., 2009., 2838.],\n",
       "       [5393., 4517.,    0., 8253., 3543., 4510., 2415., 2520., 3597.],\n",
       "       [8643., 7779., 8253.,    0., 6709., 7609., 5807., 5800., 6717.],\n",
       "       [3937., 2987., 3543., 6709.,    0., 2894.,  950.,  926., 1850.],\n",
       "       [4884., 3822., 4510., 7609., 2894.,    0., 1897., 1952., 2912.],\n",
       "       [2818., 1994., 2415., 5807.,  950., 1897.,    0.,    0.,  934.],\n",
       "       [2918., 2009., 2520., 5800.,  926., 1952.,    0.,    0.,  979.],\n",
       "       [3798., 2838., 3597., 6717., 1850., 2912.,  934.,  979.,    0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Part 3, finding the maximum likelihood estimate of the transition matrix\n",
    "# a numpy array of size (n_cities,n_cities)\n",
    "# The transition matrix should be ordered in such a way that\n",
    "# p_{'Aracaju (SE)','Rio de Janeiro (RJ)'} = transition_matrix[cityToIndex['Aracaju (SE)'],cityToIndex['Rio de Janeiro (RJ)']]\n",
    "# and represents the probability of travelling Aracaju (SE)->Rio de Janeiro (RJ)\n",
    "# Make sure that the transition_matrix does not contain np.nan from division by zero for instance\n",
    "\n",
    "frequency_counts = np.zeros((n_cities, n_cities))\n",
    "for (from_city, to_city), count in transition_counts.items():\n",
    "    i = cityToIndex[from_city]\n",
    "    j = cityToIndex[to_city]\n",
    "    frequency_counts[i, j] = count\n",
    "\n",
    "frequency_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcc57404-e681-4587-9097-13343462bf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37224.],\n",
       "       [30779.],\n",
       "       [34748.],\n",
       "       [57317.],\n",
       "       [23796.],\n",
       "       [30480.],\n",
       "       [16815.],\n",
       "       [17104.],\n",
       "       [23625.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums = frequency_counts.sum(axis=1, keepdims=True)\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e8994a4-a48d-4f3c-bd46-cd565c27b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = np.divide(\n",
    "    frequency_counts, row_sums, where=row_sums != 0  # Avoid division by zero\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10bb2fbe-7ac9-456f-ba45-77a66e19a4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.00000000e+00, -3.14924841e-01,  1.11731432e-04, -5.29756651e-02,\n",
       "        -7.76523120e-02, -1.58563203e-01, -1.46602836e-01, -1.25937067e-01,\n",
       "        -1.23455807e-01]),\n",
       " array([[-0.38283116,  0.08762827, -0.01318524,  0.02786052,  0.04059845,\n",
       "          0.87008992, -0.26240249, -0.11656833,  0.01472944],\n",
       "        [-0.31654739, -0.07463034,  0.00735186, -0.02386169,  0.07738492,\n",
       "         -0.05028166, -0.10514477,  0.93162534,  0.02646293],\n",
       "        [-0.35736667,  0.02135482, -0.01857522, -0.06160465, -0.0372502 ,\n",
       "          0.11008305,  0.92257216, -0.0076412 , -0.01950291],\n",
       "        [-0.58947812,  0.89277833,  0.02244795,  0.25287422, -0.01521697,\n",
       "         -0.41177574, -0.18491201, -0.18511324,  0.26081595],\n",
       "        [-0.24473056, -0.19290546,  0.02003527, -0.53150481,  0.67900306,\n",
       "         -0.13036366, -0.07244699, -0.19371104,  0.20403156],\n",
       "        [-0.31347232, -0.07380937, -0.00837202, -0.08186406, -0.02571517,\n",
       "         -0.15033461, -0.12096819, -0.12987105, -0.90627714],\n",
       "        [-0.17293429, -0.23451106,  0.70995875,  0.44468668, -0.00925568,\n",
       "         -0.06529505, -0.02603764, -0.09789886,  0.09543442],\n",
       "        [-0.17590652, -0.23403616, -0.70294435,  0.45767142,  0.01753121,\n",
       "         -0.08031243, -0.03759005, -0.09713589,  0.10731006],\n",
       "        [-0.2429719 , -0.19186904, -0.01671699, -0.48425763, -0.72707962,\n",
       "         -0.09180982, -0.11307001, -0.10368572,  0.21699569]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should be a numpy array of length n_cities which sums to 1 and is all positive\n",
    "\n",
    "evals,evecs = np.linalg.eig(transition_matrix.T)\n",
    "stationary_distribution_problem5 = evecs[:,0]/np.sum(evecs[:,0])\n",
    "\n",
    "evals,evecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "434f7d59-f071-443d-a4de-0d93249b194a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13690932, 0.1132047 , 0.12780262, 0.21081107, 0.08752133,\n",
       "       0.11210498, 0.06184532, 0.06290826, 0.0868924 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evecs[:,0]/np.sum(evecs[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0886bef0-c9fb-4250-8de6-b2648e624e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.13331717737273135)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import matrix_power\n",
    "# Compute the return probability for part 3 of problem 5\n",
    "\n",
    "# Initial state vector (start in 'Aracaju (SE)')\n",
    "initial_state = np.zeros(n_cities)\n",
    "initial_state[cityToIndex['Aracaju (SE)']] = 1\n",
    "\n",
    "# Compute the state vector after 3 steps\n",
    "pi_3 = initial_state @ matrix_power(transition_matrix, 3)\n",
    "return_probability_problem5 = pi_3[cityToIndex['Aracaju (SE)']]\n",
    "return_probability_problem5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3b3c85e2-69df-4a73-9bbb-3c6125de7a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aracaju (SE)->Campo Grande (MS)->Brasilia (DF)->Rio de Janeiro (RJ)->Aracaju (SE)->Brasilia (DF)->Florianopolis (SC)->Rio de Janeiro (RJ)->Recife (PE)->Brasilia (DF)->"
     ]
    }
   ],
   "source": [
    "# Once you have created all your functions, you can make a small test here to see\n",
    "# what would be generated from your model.\n",
    "import numpy as np\n",
    "start = np.zeros(shape=(n_cities,1))\n",
    "start[cityToIndex['Aracaju (SE)'],0] = 1\n",
    "current_pos = start\n",
    "for i in range(10):\n",
    "    random_word_index = np.random.choice(range(n_cities),p=current_pos.reshape(-1))\n",
    "    current_pos = np.zeros_like(start)\n",
    "    current_pos[random_word_index] = 1\n",
    "    print(indexToCity[random_word_index],end='->')\n",
    "    current_pos = (current_pos.T@transition_matrix).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a4bad-03f3-4a02-9022-967cffb0c6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91908aef-c9c9-483c-9cc2-0580d728b896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb68a06-658d-4498-b027-f1e8e0faedba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f67fc657-0c92-4fec-875a-88fffa0f1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming sms_data is already loaded as (text, label) tuples\n",
    "# Prepare a simple dataset (e.g., with word counts as features) for demonstration\n",
    "# Replace this with the actual dataset preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert text to feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([text for text, _ in sms_data])\n",
    "Y = np.array([label for _, label in sms_data])\n",
    "\n",
    "# Split the dataset into train and test sets (70-30 split)\n",
    "X_train_problem6, X_test_problem6, Y_train_problem6, Y_test_problem6 = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train a kNN model with k=4\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_problem6, Y_train_problem6)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions_problem6 = knn_model.predict(X_test_problem6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1fe4d18c-7ea1-41e4-801f-312ddc3bcaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_problem6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e2a59af8-4fb8-4493-947c-e5948b088aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0000\n",
      "95% CI for Precision: [0.9701, 1.0299]\n"
     ]
    }
   ],
   "source": [
    "# Compute precision\n",
    "problem6_precision = precision_score(Y_test_problem6, predictions_problem6, pos_label=1)\n",
    "\n",
    "# Compute Hoeffding's confidence interval for precision\n",
    "N_precision = len(Y_test_problem6)\n",
    "confidence = 0.95\n",
    "delta = 1 - confidence\n",
    "problem6_precision_l = np.sqrt(-np.log(delta) / (2 * N_precision))\n",
    "\n",
    "# Confidence interval\n",
    "precision_ci_lower = problem6_precision - problem6_precision_l\n",
    "precision_ci_upper = problem6_precision + problem6_precision_l\n",
    "\n",
    "print(f\"Precision: {problem6_precision:.4f}\")\n",
    "print(f\"95% CI for Precision: [{precision_ci_lower:.4f}, {precision_ci_upper:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7441ce-a1ab-48d7-8aee-010e06b96530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-env",
   "language": "python",
   "name": "data-science-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
